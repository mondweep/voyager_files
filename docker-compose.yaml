services:
  llamacpp:
    container_name: ${INFERENCE_ENG}
    image: gclub/llama.cpp:${INFERENCE_ENG_VERSION}
    restart: always
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
          cpus: "${NUM_CPU_CORES}"
    volumes:
      - "${DOCKER_VOLUME_DIRECTORY:-.}/${MODEL_SAVE_PATH}:/models"
    expose:
      - ${INFERENCE_ENG_PORT}
    ports:
      - ${INFERENCE_ENG_PORT}:${INFERENCE_ENG_PORT}
    command: ["-m", "models/${LANGUAGE_MODEL_NAME}","-t","${NUM_THREADS_COUNT}","-c","8192"]

  embedding_eng:
    container_name: ${EMBEDDING_ENG}
    image: gclub/llama.cpp:${INFERENCE_ENG_VERSION}
    restart: always
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
          cpus: "${NUM_CPU_CORES_EMBEDDING}"
    volumes:
      - "${DOCKER_VOLUME_DIRECTORY:-.}/${MODEL_SAVE_PATH}:/models"
    expose:
      - ${EMBEDDING_ENG_PORT}
    ports:
      - ${EMBEDDING_ENG_PORT}:${EMBEDDING_ENG_PORT}
    command: ["-m", "models/${EMBEDDING_MODEL_NAME}","--embeddings","--pooling","mean","-c","512"]
  
  voyager:
    container_name: voyager
    restart: always
    build:
      dockerfile: Dockerfile
      context: .
    volumes:
      - ${DATABASE_BIND_PATH}:/tmp/lancedb
      - ./files:/app/files
    expose:
      - ${APP_PORT}
    ports:
      - ${APP_PORT}:${APP_PORT}
    depends_on:
      - llamacpp
      - embedding_eng
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    tty: true
    stdin_open: true